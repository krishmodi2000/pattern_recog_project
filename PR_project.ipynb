{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJIRFZJE722e"
   },
   "source": [
    "# Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEmPP5QGYOFs"
   },
   "source": [
    "We import all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YFFIWXE_1Jqw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QKf-Kcy8fDi"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1rexSsYXiun"
   },
   "source": [
    "The dataset used has 372450 images of alphabets of 28×28 pixels, as a CSV file: https://www.kaggle.com/sachinpatel21/az-handwritten-alphabets-in-csv-format. The CSV file is in the same folder as the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PARSCivX5Tu"
   },
   "source": [
    "# Read the data\n",
    "\n",
    "We read the csv file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "SZ36HUen1zZf",
    "outputId": "e55aeed4-8052-4894-f5c6-462fb3f5c3b3"
   },
   "outputs": [],
   "source": [
    "file = 'project_dataset.csv'\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRKvL9Uk-k6-"
   },
   "source": [
    "# Splitting data into their images and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJ8BnpW8YmYz"
   },
   "source": [
    "We need to split the data into images and their respective labels.\n",
    "The ‘0’ column contains the labels, so we drop it from the data dataframe and use it separately as y to form the labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "J9XJInkiSzSv",
    "outputId": "b5ef6e02-5112-4c84-d7b5-49a9c8684049"
   },
   "outputs": [],
   "source": [
    "x = data.drop('0',axis = 1)\n",
    "y = data['0']\n",
    "\n",
    "display(x.head(5))\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_B029zb_9-O"
   },
   "source": [
    "# Splitting and Reshaping data\n",
    "\n",
    "*   Then we split the data into training & testing dataset using train_test_split().\n",
    "*   We reshape the data into the orignal 28x28 pixels image, as the CSV file contains the data for the values of pixels in 784 columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fu_b0Av2TdU2",
    "outputId": "ec607e5e-96ec-405b-d6d4-b6a222afc6b5"
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2)\n",
    "\n",
    "train_x = np.reshape(train_x.values, (train_x.shape[0], 28,28))\n",
    "test_x = np.reshape(test_x.values, (test_x.shape[0], 28,28))\n",
    "\n",
    "print(\"Train data shape: \", train_x.shape)\n",
    "print(\"Test data shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTgfsXc_A4ep"
   },
   "source": [
    "# Putting integers as labels for each alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a35l9wuGY6u2"
   },
   "source": [
    "All the labels are present in the form of floating point values, that we convert to integer values. So we create a dictionary to map the integer values with the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRShF1NGTvzs"
   },
   "outputs": [],
   "source": [
    "word_dict = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',\n",
    "             11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',\n",
    "             20:'U',21:'V',22:'W',23:'X', 24:'Y',25:'Z'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c654sZeNDX1c"
   },
   "source": [
    "# Showing the image count of each alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SghuUPgwZFJi"
   },
   "source": [
    "Here we are only describing the distribution of the alphabets.\n",
    "Firstly, we convert the labels into integer values and append them into the count list according to the label. This count list has the number of images present in the dataset belonging to each alphabet.\n",
    "Now we create a list of alphabets containing all the characters using the values() function of the dictionary. Using the count & alphabets lists we draw the horizontal bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I_Y6cUa1UYeq",
    "outputId": "2d4dac2d-6d50-4719-833c-26a8b7e70cdd"
   },
   "outputs": [],
   "source": [
    "y_int = np.int0(y)\n",
    "count = np.zeros(26, dtype='int')\n",
    "\n",
    "for i in y_int:\n",
    "    count[i] +=1\n",
    "\n",
    "alphabets = []\n",
    "\n",
    "for i in word_dict.values():\n",
    "    alphabets.append(i)\n",
    "\n",
    "total_count = {alphabets[i]: count[i] for i in range(len(alphabets))}\n",
    "\n",
    "display(total_count)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "ax.barh(alphabets, count)\n",
    "\n",
    "plt.xlabel(\"Number of elements \")\n",
    "plt.ylabel(\"Alphabets\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bQZDvjVFdIy"
   },
   "source": [
    "# Randomly plotting 9 images as an example for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCKljGg-ZXql"
   },
   "source": [
    "Now we shuffle some of the images of the train set. The shuffling is done using the shuffle() function so that we can display some random images. We then create 9 plots in 3×3 shape & display the thresholded images of 9 alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "BNVPAHqrUmyZ",
    "outputId": "04a5bc7a-3a9c-4da5-e0ed-71d637366238"
   },
   "outputs": [],
   "source": [
    "shuff = shuffle(train_x[:100])\n",
    "shuff = shuff.astype(np.uint8)\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (10,10))\n",
    "axes = ax.flatten()\n",
    "\n",
    "for i in range(9):\n",
    "    a, shu = cv2.threshold(shuff[i], 30, 200, cv2.THRESH_BINARY)\n",
    "    axes[i].imshow(np.reshape(shu, (28,28)), cmap=\"Greys\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXXcVdKVGgEE"
   },
   "source": [
    "# Reshaping the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZPzlsS-bArq"
   },
   "source": [
    "Now we reshape the train & test image dataset so that they can be put in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DpNaF8bAI3e",
    "outputId": "b6bd47d2-faee-4cb1-a5fa-d708a4dc2f44"
   },
   "outputs": [],
   "source": [
    "train_X = train_x.reshape(train_x.shape[0],train_x.shape[1],train_x.shape[2],1)\n",
    "print(\"New shape of train data: \", train_X.shape)\n",
    "\n",
    "test_X = test_x.reshape(test_x.shape[0], test_x.shape[1], test_x.shape[2],1)\n",
    "print(\"New shape of train data: \", test_X.shape)\n",
    "\n",
    "train_y_cat = to_categorical(train_y, num_classes = 26, dtype='int')\n",
    "print(\"New shape of train labels: \", train_y_cat.shape)\n",
    "\n",
    "test_y_cat = to_categorical(test_y, num_classes = 26, dtype='int')\n",
    "print(\"New shape of test labels: \", test_y_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDqfi-xPTdtp"
   },
   "source": [
    "# Model Designing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbvTSRqXbFN-"
   },
   "source": [
    "CNN stands for Convolutional Neural Networks that are used to extract the features of the images using several layers of filters.\n",
    "The convolution layers are generally followed by maxpool layers that are used to reduce the number of features extracted and ultimately the output of the maxpool and layers and convolution layers are flattened into a vector of single dimension and are given as an input to the Dense layer (The fully connected network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PaET2kaFDq3"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "\n",
    "model.add(Dense(26,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjbFoxxObRa3"
   },
   "source": [
    "# Model Compilation and Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_tFnu6_bYP8"
   },
   "source": [
    "* Here we are compiling the model, where we define the optimizing function & the loss function to be used for fitting.\n",
    "* The optimizing function used is Adam, that is a combination of RMSprop & Adagram optimizing algorithms.\n",
    "* The dataset is very large so we are training for only a single epoch.\n",
    "* Then we get the model summary that tells us what were the different layers defined in the model & also we save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7X4wjTEfAsS",
    "outputId": "cb96fc70-c096-4a1c-d6eb-830eb4126489",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_X, train_y_cat, epochs=20, batch_size = 500, validation_data = (test_X,test_y_cat), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sB_puqL0saRq",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.save(r'model_hand.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBLTuWWpohgo"
   },
   "source": [
    "We print out the training & validation accuracies along with the training & validation losses for character recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAQvnUIciCFd"
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWDe7qeKop14"
   },
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRwL6KTkos85"
   },
   "source": [
    "We create 9 subplots of (3,3) shape & visualize some of the test dataset alphabets along with their predictions that are made using the model.predict() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-joFgy_iMKh"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,3, figsize=(8,9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i,ax in enumerate(axes):\n",
    "    img = np.reshape(test_X[i], (28,28))\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    \n",
    "    pred = word_dict[np.argmax(test_y_cat[i])]\n",
    "    ax.set_title(\"Prediction: \"+pred)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v95B1rA2o71A"
   },
   "source": [
    "# Predicting using an external image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMmHAti4o_s6"
   },
   "source": [
    "We read an external image that is originally an image of the alphabet ‘B’ and make a copy of it to process and feed into the model for the prediction.\n",
    "The img read is then converted from BGR representation (as OpenCV reads the image in BGR format) to RGB for displaying the image, and is resized to our required dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ha32U9b5jKAp"
   },
   "outputs": [],
   "source": [
    "file2 = 'char.jpg'\n",
    "img = cv2.imread(file2)\n",
    "img_copy = img.copy()\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (400,440))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5hnOvl0pH-q"
   },
   "source": [
    "Now we do some processing on the copied image (img_copy). We convert the image from BGR to grayscale and apply thresholding to it to keep the image smooth without any sort of hazy gray colors in the image that could lead to wrong predictions.\n",
    "The image is to be then resized using cv2.resize() function into the dimensions that the model takes as input, along with reshaping the image using np.reshape() so that it can be used as model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vebRbGr0kxMr"
   },
   "outputs": [],
   "source": [
    "img_copy = cv2.GaussianBlur(img_copy, (7,7), 0)\n",
    "img_gray = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n",
    "_, img_thresh = cv2.threshold(img_gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "img_final = cv2.resize(img_thresh, (28,28))\n",
    "img_final =np.reshape(img_final, (1,28,28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T_30Gpnp22p"
   },
   "source": [
    "We make a prediction using the processed image & use the np.argmax() function to get the index of the class with the highest predicted probability. Using this we get to know the exact character through the word_dict dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B-2TDbjMk3Ub"
   },
   "outputs": [],
   "source": [
    "word_dict = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',\n",
    "             11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',\n",
    "             20:'U',21:'V',22:'W',23:'X', 24:'Y',25:'Z'}\n",
    "\n",
    "model1 = keras.models.load_model(\"model_hand.h5\")\n",
    "\n",
    "img_pred = word_dict[np.argmax(model1.predict(img_final))]\n",
    "\n",
    "cv2.putText(img, \"Prediction: \" + img_pred, (20,410), cv2.FONT_HERSHEY_DUPLEX, 1.3, color = (255,0,30))\n",
    "cv2.imshow('test',img)\n",
    "\n",
    "while (1):\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PR project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
